import os
import time
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================================
# CONFIGURA√á√ïES
# ==========================================================
BASE_URL = "https://transitfeeds.com/p/carris/1000"
DOWNLOAD_DIR = "carris_gtfs"
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; AutominingScript/3.1)"}
MAX_PAGES = 100       # limite de seguran√ßa (~86 p√°ginas)
MAX_WORKERS = 6       # n√∫mero de downloads simult√¢neos
TIMEOUT = 30          # timeout por requisi√ß√£o (segundos)
RETRIES = 3           # n√∫mero de tentativas por arquivo

# ==========================================================
# FUN√á√ïES AUXILIARES
# ==========================================================
def ensure_dir(path):
    os.makedirs(path, exist_ok=True)

def get_page(url):
    """Obt√©m o HTML da p√°gina com tratamento de erro."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao carregar {url}: {e}")
        return None

def sanitize_filename(name):
    """Remove caracteres inv√°lidos e normaliza o nome do arquivo."""
    name = re.sub(r"[^\w\-]", "_", name)
    return name.strip("_")

def parse_versions(html):
    """Extrai lista de vers√µes e links de download."""
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", {"class": "table"})
    if not table:
        return []

    rows = table.find_all("tr")[1:]  # pula cabe√ßalho
    data = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 4:
            continue

        date = cols[0].get_text(strip=True)
        size = cols[1].get_text(strip=True)
        routes = cols[2].get_text(strip=True)
        download_link = None

        for a in cols[3].find_all("a"):
            if "Download" in a.text:
                download_link = urljoin(BASE_URL, a["href"])
                break

        if download_link:
            safe_date = sanitize_filename(date)
            filename = f"{safe_date}.zip"
            data.append({
                "date": date,
                "size": size,
                "routes": routes,
                "download": download_link,
                "filename": filename
            })
    return data

def download_file(item, max_retries=RETRIES):
    """Baixa um arquivo ZIP com verifica√ß√£o e tentativas."""
    url = item["download"]
    dest = os.path.join(DOWNLOAD_DIR, item["filename"])

    # pula se j√° existe e parece completo
    if os.path.exists(dest) and os.path.getsize(dest) > 50_000:
        return f"‚úÖ J√° existe: {item['filename']}"

    for attempt in range(1, max_retries + 1):
        try:
            with requests.get(url, headers=HEADERS, stream=True, timeout=TIMEOUT) as r:
                r.raise_for_status()
                total_length = int(r.headers.get("Content-Length", 0))
                written = 0
                tmp_path = dest + ".part"

                with open(tmp_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            written += len(chunk)

                # Verifica integridade pelo tamanho baixado
                if total_length and abs(written - total_length) > 1024:
                    raise IOError(f"Tamanho inconsistente: esperado {total_length}, baixado {written}")

                os.replace(tmp_path, dest)
                return f"üíæ OK: {item['filename']} ({written/1024:.1f} KB)"

        except Exception as e:
            print(f"‚ö†Ô∏è Tentativa {attempt} falhou em {item['filename']}: {e}")
            time.sleep(2)

    return f"‚ùå Falha ap√≥s {max_retries} tentativas: {item['filename']}"

# ==========================================================
# EXECU√á√ÉO PRINCIPAL
# ==========================================================
def main():
    ensure_dir(DOWNLOAD_DIR)
    all_data = []
    page = 1
    start_time = time.time()

    print("üîç Iniciando scraping das vers√µes GTFS da Carris...\n")

    while page <= MAX_PAGES:
        url = f"{BASE_URL}?p={page}"
        html = get_page(url)
        if not html:
            break

        data = parse_versions(html)
        if not data:
            print(f"‚ùå Nenhum dado encontrado na p√°gina {page}. Fim da lista.")
            break

        print(f"üìÑ P√°gina {page}: {len(data)} vers√µes encontradas.")
        all_data.extend(data)
        page += 1
        time.sleep(0.4)  # pequena pausa entre p√°ginas

    print(f"\nüì¶ Total de vers√µes encontradas: {len(all_data)}\n")

    # ======================================================
    # DOWNLOAD PARALELO
    # ======================================================
    print("üöÄ Iniciando downloads simult√¢neos...\n")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(download_file, item): item for item in all_data}
        for future in as_completed(futures):
            print(future.result())

    total_time = time.time() - start_time
    print(f"\n‚úÖ Conclu√≠do! {len(all_data)} arquivos processados em {total_time:.1f}s.")
    print("üìÅ Pasta:", os.path.abspath(DOWNLOAD_DIR))

# ==========================================================
if __name__ == "__main__":
    main()
